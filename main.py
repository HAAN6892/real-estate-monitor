"""
ìˆ˜ë„ê¶Œ ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ëª¨ë‹ˆí„°ë§ ë´‡ v4
- êµ­í† êµí†µë¶€ ì‹¤ê±°ë˜ê°€ API ë°ì´í„° ìˆ˜ì§‘
- ì¹´ì¹´ì˜¤ ë¡œì»¬ APIë¡œ ì•„íŒŒíŠ¸ ì¢Œí‘œ â†’ ìµœê·¼ì ‘ ì—­ ê±°ë¦¬ ê³„ì‚°
- ë™ì¼ ë‹¨ì§€ ë¬¶ê¸°, ê°€ê²©ëŒ€ë³„ ê·¸ë£¹í•‘, í‰ë‹¹ê°€ ê³„ì‚°
- ì§€ì—­ë³„ ìš”ì•½ í…”ë ˆê·¸ë¨ ì•Œë¦¼
- [v4] data.json íŒŒì¼ë¡œ ë§¤ë¬¼ ë°ì´í„° ì €ì¥ (ë…¸ì…˜ ëŒ€ì²´)
"""

import json
import math
import os
import requests
import urllib.parse
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from pathlib import Path

# â”€â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€
BASE_DIR = Path(__file__).parent
CONFIG_PATH = BASE_DIR / "config.json"
HISTORY_PATH = BASE_DIR / "sent_history.json"
COORD_CACHE_PATH = BASE_DIR / "coord_cache.json"
APT_INFO_CACHE_PATH = BASE_DIR / "apt_info_cache.json"
DATA_JSON_PATH = BASE_DIR / "data.json"

# â”€â”€â”€ ì‹ ë¶„ë‹¹ì„  + ì£¼ìš” ì§€í•˜ì² ì—­ ì¢Œí‘œ â”€â”€â”€
STATIONS = [
    # ì‹ ë¶„ë‹¹ì„ 
    {"name": "ê°•ë‚¨", "lat": 37.4979, "lon": 127.0276, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ì–‘ì¬", "lat": 37.4842, "lon": 127.0353, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ì–‘ì¬ì‹œë¯¼ì˜ìˆ²", "lat": 37.4700, "lon": 127.0386, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ì²­ê³„ì‚°ì…êµ¬", "lat": 37.4474, "lon": 127.0562, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "íŒêµ", "lat": 37.3948, "lon": 127.1112, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ì •ì", "lat": 37.3669, "lon": 127.1085, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ë¯¸ê¸ˆ", "lat": 37.3510, "lon": 127.1095, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ë™ì²œ", "lat": 37.3383, "lon": 127.1085, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ìˆ˜ì§€êµ¬ì²­", "lat": 37.3220, "lon": 127.0960, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ì„±ë³µ", "lat": 37.3114, "lon": 127.0786, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ìƒí˜„", "lat": 37.3005, "lon": 127.0653, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ê´‘êµì¤‘ì•™", "lat": 37.2886, "lon": 127.0492, "line": "ì‹ ë¶„ë‹¹ì„ "},
    {"name": "ê´‘êµ", "lat": 37.2831, "lon": 127.0446, "line": "ì‹ ë¶„ë‹¹ì„ "},
    # ë¶„ë‹¹ì„  ì£¼ìš”ì—­
    {"name": "ì•¼íƒ‘", "lat": 37.4112, "lon": 127.1272, "line": "ë¶„ë‹¹ì„ "},
    {"name": "ì´ë§¤", "lat": 37.3952, "lon": 127.1275, "line": "ë¶„ë‹¹ì„ "},
    {"name": "ì„œí˜„", "lat": 37.3845, "lon": 127.1237, "line": "ë¶„ë‹¹ì„ "},
    {"name": "ìˆ˜ë‚´", "lat": 37.3775, "lon": 127.1155, "line": "ë¶„ë‹¹ì„ "},
    {"name": "ì˜¤ë¦¬", "lat": 37.3397, "lon": 127.1090, "line": "ë¶„ë‹¹ì„ "},
    {"name": "ì£½ì „", "lat": 37.3249, "lon": 127.1076, "line": "ë¶„ë‹¹ì„ "},
    {"name": "ë³´ì •", "lat": 37.3127, "lon": 127.1084, "line": "ë¶„ë‹¹ì„ "},
    {"name": "êµ¬ì„±", "lat": 37.3005, "lon": 127.1085, "line": "ë¶„ë‹¹ì„ "},
    {"name": "ëª¨ë€", "lat": 37.4321, "lon": 127.1293, "line": "ë¶„ë‹¹ì„ "},
    {"name": "íƒœí‰", "lat": 37.4431, "lon": 127.1268, "line": "ë¶„ë‹¹ì„ "},
    # 8í˜¸ì„  (ì†¡íŒŒ/ê°•ë™)
    {"name": "ì ì‹¤", "lat": 37.5133, "lon": 127.1001, "line": "2í˜¸ì„ "},
    {"name": "ì„ì´Œ", "lat": 37.5056, "lon": 127.1070, "line": "8í˜¸ì„ "},
    {"name": "ì†¡íŒŒ", "lat": 37.5014, "lon": 127.1125, "line": "8í˜¸ì„ "},
    {"name": "ê°€ë½ì‹œì¥", "lat": 37.4926, "lon": 127.1183, "line": "8í˜¸ì„ "},
    {"name": "ë¬¸ì •", "lat": 37.4857, "lon": 127.1228, "line": "8í˜¸ì„ "},
    {"name": "ì¥ì§€", "lat": 37.4784, "lon": 127.1264, "line": "8í˜¸ì„ "},
    {"name": "ë³µì •", "lat": 37.4706, "lon": 127.1265, "line": "8í˜¸ì„ "},
    {"name": "ì‚°ì„±", "lat": 37.4573, "lon": 127.1498, "line": "8í˜¸ì„ "},
    {"name": "ë‚¨í•œì‚°ì„±ì…êµ¬", "lat": 37.4502, "lon": 127.1578, "line": "8í˜¸ì„ "},
    {"name": "ë‹¨ëŒ€ì˜¤ê±°ë¦¬", "lat": 37.4441, "lon": 127.1565, "line": "8í˜¸ì„ "},
    # 5í˜¸ì„  (ê°•ë™/í•˜ë‚¨)
    {"name": "ê°•ë™", "lat": 37.5354, "lon": 127.1320, "line": "5í˜¸ì„ "},
    {"name": "ë‘”ì´Œë™", "lat": 37.5271, "lon": 127.1366, "line": "5í˜¸ì„ "},
    {"name": "ì˜¬ë¦¼í”½ê³µì›", "lat": 37.5165, "lon": 127.1312, "line": "5í˜¸ì„ "},
    {"name": "ë°©ì´", "lat": 37.5084, "lon": 127.1268, "line": "5í˜¸ì„ "},
    {"name": "ë¯¸ì‚¬", "lat": 37.5608, "lon": 127.1900, "line": "5í˜¸ì„ "},
    {"name": "í•˜ë‚¨í’ì‚°", "lat": 37.5519, "lon": 127.2048, "line": "5í˜¸ì„ "},
    {"name": "í•˜ë‚¨ì‹œì²­", "lat": 37.5393, "lon": 127.2149, "line": "5í˜¸ì„ "},
    {"name": "í•˜ë‚¨ê²€ë‹¨ì‚°", "lat": 37.5249, "lon": 127.2242, "line": "5í˜¸ì„ "},
    # 4í˜¸ì„  (ê³¼ì²œ/ì•ˆì–‘)
    {"name": "ê³¼ì²œ", "lat": 37.4340, "lon": 126.9877, "line": "4í˜¸ì„ "},
    {"name": "ì •ë¶€ê³¼ì²œì²­ì‚¬", "lat": 37.4265, "lon": 126.9899, "line": "4í˜¸ì„ "},
    {"name": "ì¸ë•ì›", "lat": 37.4175, "lon": 126.9892, "line": "4í˜¸ì„ "},
    {"name": "í‰ì´Œ", "lat": 37.3947, "lon": 126.9635, "line": "4í˜¸ì„ "},
    {"name": "ë²”ê³„", "lat": 37.3898, "lon": 126.9515, "line": "4í˜¸ì„ "},
    {"name": "ê¸ˆì •", "lat": 37.3717, "lon": 126.9416, "line": "4í˜¸ì„ "},
    # 3í˜¸ì„  (ê°•ë‚¨/ì„œì´ˆ)
    {"name": "êµëŒ€", "lat": 37.4937, "lon": 127.0146, "line": "3í˜¸ì„ "},
    {"name": "ë‚¨ë¶€í„°ë¯¸ë„", "lat": 37.4856, "lon": 127.0148, "line": "3í˜¸ì„ "},
    {"name": "ì–‘ì¬", "lat": 37.4842, "lon": 127.0353, "line": "3í˜¸ì„ "},
    {"name": "ë§¤ë´‰", "lat": 37.4872, "lon": 127.0473, "line": "3í˜¸ì„ "},
    {"name": "ë„ê³¡", "lat": 37.4915, "lon": 127.0553, "line": "3í˜¸ì„ "},
    {"name": "ëŒ€ì¹˜", "lat": 37.4948, "lon": 127.0628, "line": "3í˜¸ì„ "},
    {"name": "í•™ì—¬ìš¸", "lat": 37.4969, "lon": 127.0713, "line": "3í˜¸ì„ "},
    {"name": "ëŒ€ì²­", "lat": 37.4921, "lon": 127.0818, "line": "3í˜¸ì„ "},
    {"name": "ì¼ì›", "lat": 37.4837, "lon": 127.0876, "line": "3í˜¸ì„ "},
    {"name": "ìˆ˜ì„œ", "lat": 37.4870, "lon": 127.1018, "line": "3í˜¸ì„ "},
    # 2í˜¸ì„  (ê°•ë‚¨)
    {"name": "ì—­ì‚¼", "lat": 37.5006, "lon": 127.0365, "line": "2í˜¸ì„ "},
    {"name": "ì„ ë¦‰", "lat": 37.5045, "lon": 127.0490, "line": "2í˜¸ì„ "},
    {"name": "ì‚¼ì„±", "lat": 37.5088, "lon": 127.0631, "line": "2í˜¸ì„ "},
    {"name": "ì¢…í•©ìš´ë™ì¥", "lat": 37.5108, "lon": 127.0735, "line": "2í˜¸ì„ "},
    # ê²½ê°•ì„  (ê´‘ì£¼)
    {"name": "ì´ˆì›”", "lat": 37.3702, "lon": 127.2810, "line": "ê²½ê°•ì„ "},
    {"name": "ê³¤ì§€ì•”", "lat": 37.3381, "lon": 127.3230, "line": "ê²½ê°•ì„ "},
    {"name": "ì‹ ë‘”ë„ì˜ˆì´Œ", "lat": 37.3194, "lon": 127.3651, "line": "ê²½ê°•ì„ "},
    {"name": "ì´ì²œ", "lat": 37.2750, "lon": 127.4433, "line": "ê²½ê°•ì„ "},
    {"name": "ê²½ê¸°ê´‘ì£¼", "lat": 37.4090, "lon": 127.2540, "line": "ê²½ê°•ì„ "},
]


def load_config():
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


def load_history():
    if HISTORY_PATH.exists():
        with open(HISTORY_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def save_history(history):
    history = history[-3000:]
    with open(HISTORY_PATH, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def load_coord_cache():
    if COORD_CACHE_PATH.exists():
        with open(COORD_CACHE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_coord_cache(cache):
    with open(COORD_CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)


def load_apt_info_cache():
    if APT_INFO_CACHE_PATH.exists():
        with open(APT_INFO_CACHE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_apt_info_cache(cache):
    with open(APT_INFO_CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)


# â”€â”€â”€ data.json ë¡œë“œ/ì €ì¥ â”€â”€â”€
def load_data_json():
    """ê¸°ì¡´ data.json ë¡œë“œ (ì—†ìœ¼ë©´ ë¹ˆ êµ¬ì¡° ë°˜í™˜)"""
    if DATA_JSON_PATH.exists():
        with open(DATA_JSON_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"updated_at": "", "properties": []}


def save_data_json(data):
    """data.json ì €ì¥"""
    with open(DATA_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"  [data.json] ì €ì¥ ì™„ë£Œ ({len(data['properties'])}ê±´)")


def fetch_region_apt_list(api_key, sigungu_code, apt_list_cache):
    if sigungu_code in apt_list_cache:
        return apt_list_cache[sigungu_code]

    url = "https://apis.data.go.kr/1613000/AptListService3/getSigunguAptList3"
    all_items = []
    page = 1

    while True:
        params = {
            "serviceKey": api_key,
            "sigunguCode": sigungu_code,
            "numOfRows": "200",
            "pageNo": str(page),
            "type": "json"
        }
        try:
            resp = requests.get(url, params=params, timeout=15)
            data = resp.json()
            items = data.get("response", {}).get("body", {}).get("items", [])
            if not items:
                break
            if isinstance(items, dict):
                items = [items]
            all_items.extend(items)
            total = data.get("response", {}).get("body", {}).get("totalCount", 0)
            if len(all_items) >= total:
                break
            page += 1
        except Exception as e:
            print(f"    [ëª©ë¡ì¡°íšŒ ì‹¤íŒ¨] {sigungu_code}: {e}")
            break

    apt_list_cache[sigungu_code] = all_items
    print(f"    [ëª©ë¡] {sigungu_code}: {len(all_items)}ê°œ ë‹¨ì§€ ë¡œë“œ")
    return all_items


def find_kapt_code(apt_name, apt_list):
    def clean(name):
        return name.replace(" ", "").replace("(", "").replace(")", "").lower()

    clean_name = clean(apt_name)

    for apt in apt_list:
        if clean(apt.get("kaptName", "")) == clean_name:
            return apt["kaptCode"]

    for apt in apt_list:
        kname = clean(apt.get("kaptName", ""))
        if clean_name in kname or kname in clean_name:
            return apt["kaptCode"]

    name_words = [w for w in clean_name if len(w) >= 2]
    best_score = 0
    best_code = None
    for apt in apt_list:
        kname = clean(apt.get("kaptName", ""))
        score = sum(1 for w in name_words if w in kname)
        if score > best_score:
            best_score = score
            best_code = apt["kaptCode"]

    if best_score >= 2:
        return best_code

    return None


def get_apt_household_count(api_key, apt_name, sigungu_code, apt_info_cache, apt_list_cache):
    cache_key = f"{sigungu_code}_{apt_name}"
    if cache_key in apt_info_cache:
        return apt_info_cache[cache_key]

    apt_list = fetch_region_apt_list(api_key, sigungu_code, apt_list_cache)
    kapt_code = find_kapt_code(apt_name, apt_list)

    if not kapt_code:
        print(f"    [ì„¸ëŒ€ìˆ˜] {apt_name}: ë‹¨ì§€ì½”ë“œ ëª» ì°¾ìŒ")
        result = {"ì„¸ëŒ€ìˆ˜": 0, "ë‹¨ì§€ì½”ë“œ": ""}
        apt_info_cache[cache_key] = result
        return result

    url = "https://apis.data.go.kr/1613000/AptBasisInfoServiceV4/getAphusBassInfoV4"
    params = {
        "serviceKey": api_key,
        "kaptCode": kapt_code,
        "type": "json"
    }

    try:
        resp = requests.get(url, params=params, timeout=10)
        data = resp.json()
        item = data.get("response", {}).get("body", {}).get("item", {})
        household = int(float(item.get("kaptdaCnt", 0) or 0))
        result = {"ì„¸ëŒ€ìˆ˜": household, "ë‹¨ì§€ì½”ë“œ": kapt_code}
        apt_info_cache[cache_key] = result
        print(f"    [ì„¸ëŒ€ìˆ˜] {apt_name} â†’ {household}ì„¸ëŒ€")
        return result

    except Exception as e:
        print(f"    [ì„¸ëŒ€ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨] {apt_name}: {e}")

    result = {"ì„¸ëŒ€ìˆ˜": 0, "ë‹¨ì§€ì½”ë“œ": kapt_code}
    apt_info_cache[cache_key] = result
    return result


def fetch_trades(api_key, region_code, deal_ymd):
    url = "https://apis.data.go.kr/1613000/RTMSDataSvcAptTradeDev/getRTMSDataSvcAptTradeDev"
    params = {
        "serviceKey": api_key,
        "LAWD_CD": region_code,
        "DEAL_YMD": deal_ymd,
        "pageNo": "1",
        "numOfRows": "9999"
    }

    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"  [ì˜¤ë¥˜] API í˜¸ì¶œ ì‹¤íŒ¨ ({region_code}): {e}")
        return []

    try:
        root = ET.fromstring(response.text)
    except ET.ParseError:
        print(f"  [ì˜¤ë¥˜] XML íŒŒì‹± ì‹¤íŒ¨ ({region_code})")
        return []

    result_code = root.findtext(".//resultCode")
    if result_code and result_code not in ("00", "000"):
        result_msg = root.findtext(".//resultMsg", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
        print(f"  [ì˜¤ë¥˜] API ì—ëŸ¬ ({region_code}): {result_msg}")
        return []

    items = root.findall(".//item")
    trades = []

    for item in items:
        try:
            trade = {
                "ì•„íŒŒíŠ¸": (item.findtext("aptNm") or "").strip(),
                "ë©´ì ": float(item.findtext("excluUseAr") or 0),
                "ê±°ë˜ê¸ˆì•¡": int((item.findtext("dealAmount") or "0").strip().replace(",", "")),
                "ì¸µ": int(item.findtext("floor") or 0),
                "ê±´ì¶•ë…„ë„": int(item.findtext("buildYear") or 0),
                "ê±°ë˜ë…„ë„": int(item.findtext("dealYear") or 0),
                "ê±°ë˜ì›”": int(item.findtext("dealMonth") or 0),
                "ê±°ë˜ì¼": int(item.findtext("dealDay") or 0),
                "ë²•ì •ë™": (item.findtext("umdNm") or "").strip(),
                "ì§€ë²ˆ": (item.findtext("jibun") or "").strip(),
                "ë„ë¡œëª…": (item.findtext("roadNm") or "").strip(),
            }
            trades.append(trade)
        except (ValueError, TypeError):
            continue

    return trades


def filter_trades(trades, filters):
    filtered = []
    today = datetime.now().date()
    max_days = filters.get("max_days_ago", 14)

    for t in trades:
        try:
            trade_date = datetime(t["ê±°ë˜ë…„ë„"], t["ê±°ë˜ì›”"], t["ê±°ë˜ì¼"]).date()
            if (today - trade_date).days > max_days:
                continue
        except (ValueError, TypeError):
            continue

        if t["ë©´ì "] < filters["min_area"] or t["ë©´ì "] > filters["max_area"]:
            continue
        if t["ê±°ë˜ê¸ˆì•¡"] < filters["min_price"] or t["ê±°ë˜ê¸ˆì•¡"] > filters["max_price"]:
            continue
        if t["ì¸µ"] < filters.get("min_floor", 1):
            continue
        max_by = filters.get("max_build_year", 9999)
        if max_by != 9999 and t["ê±´ì¶•ë…„ë„"] > max_by:
            continue
        filtered.append(t)
    return filtered


def make_trade_id(trade, region_name):
    return f"{region_name}_{trade['ì•„íŒŒíŠ¸']}_{trade['ë©´ì ']}_{trade['ê±°ë˜ê¸ˆì•¡']}_{trade['ì¸µ']}_{trade['ê±°ë˜ë…„ë„']}{trade['ê±°ë˜ì›”']:02d}{trade['ê±°ë˜ì¼']:02d}"


# â”€â”€â”€ ì¹´ì¹´ì˜¤ APIë¡œ ì£¼ì†Œ â†’ ì¢Œí‘œ ë³€í™˜ â”€â”€â”€
def get_coordinates(kakao_key, address, coord_cache):
    if address in coord_cache:
        return coord_cache[address]

    url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {kakao_key}"}
    params = {"query": address}

    try:
        resp = requests.get(url, headers=headers, params=params, timeout=5)
        data = resp.json()
        if data.get("documents"):
            doc = data["documents"][0]
            result = {"lat": float(doc["y"]), "lon": float(doc["x"])}
            coord_cache[address] = result
            return result
    except Exception:
        pass

    url2 = "https://dapi.kakao.com/v2/local/search/keyword.json"
    try:
        resp = requests.get(url2, headers=headers, params=params, timeout=5)
        data = resp.json()
        if data.get("documents"):
            doc = data["documents"][0]
            result = {"lat": float(doc["y"]), "lon": float(doc["x"])}
            coord_cache[address] = result
            return result
    except Exception:
        pass

    return None


def haversine(lat1, lon1, lat2, lon2):
    R = 6371
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)
    a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2
    return R * 2 * math.asin(math.sqrt(a))


def find_nearest_station(lat, lon):
    nearest = None
    min_dist = float("inf")
    for st in STATIONS:
        dist = haversine(lat, lon, st["lat"], st["lon"])
        if dist < min_dist:
            min_dist = dist
            nearest = st
    walk_min = round(min_dist * 15)
    return nearest, min_dist, walk_min


# â”€â”€â”€ í¬ë§·íŒ… í•¨ìˆ˜ â”€â”€â”€
def format_price(price_man):
    if price_man >= 10000:
        ì–µ = price_man // 10000
        ë‚˜ë¨¸ì§€ = price_man % 10000
        if ë‚˜ë¨¸ì§€ > 0:
            return f"{ì–µ}ì–µ {ë‚˜ë¨¸ì§€:,}"
        return f"{ì–µ}ì–µ"
    return f"{price_man:,}ë§Œ"


def to_pyeong(m2):
    return round(m2 / 3.3058, 1)


def price_group_label(price_man):
    ì–µ = price_man // 10000
    return f"{ì–µ}ì–µëŒ€"


# â”€â”€â”€ ë‹¨ì§€ë³„ ë¬¶ê¸° + ìš”ì•½ â”€â”€â”€
def group_by_complex(trades):
    groups = {}
    for t in trades:
        key = f"{t['ì•„íŒŒíŠ¸']}_{t['ë©´ì ']}"
        if key not in groups:
            groups[key] = {
                "ì•„íŒŒíŠ¸": t["ì•„íŒŒíŠ¸"],
                "ë©´ì ": t["ë©´ì "],
                "ê±´ì¶•ë…„ë„": t["ê±´ì¶•ë…„ë„"],
                "ë²•ì •ë™": t["ë²•ì •ë™"],
                "ë„ë¡œëª…": t.get("ë„ë¡œëª…", ""),
                "ê±°ë˜": []
            }
        groups[key]["ê±°ë˜"].append(t)
    return groups


def build_region_summary(region_name, complex_groups, kakao_key, coord_cache, sgg_name, api_key, apt_info_cache, min_households, region_code, apt_list_cache):
    """í•œ ì§€ì—­ì˜ ìš”ì•½ ë©”ì‹œì§€ ìƒì„± + data.json ì €ì¥ìš© ë°ì´í„° ë°˜í™˜"""

    summaries = []
    data_items = []  # [v4] data.json ì €ì¥ìš©
    skipped_small = 0

    for key, group in complex_groups.items():
        apt_info = get_apt_household_count(api_key, group["ì•„íŒŒíŠ¸"], region_code, apt_info_cache, apt_list_cache)
        household = apt_info["ì„¸ëŒ€ìˆ˜"]

        if household > 0 and household < min_households:
            skipped_small += 1
            continue

        trades = group["ê±°ë˜"]
        prices = [t["ê±°ë˜ê¸ˆì•¡"] for t in trades]
        min_p, max_p = min(prices), max(prices)
        avg_p = sum(prices) // len(prices)
        pyeong = to_pyeong(group["ë©´ì "])
        price_per_pyeong = round(avg_p / pyeong) if pyeong > 0 else 0

        address = f"{sgg_name} {group['ë²•ì •ë™']} {group['ì•„íŒŒíŠ¸']}"
        coord = get_coordinates(kakao_key, address, coord_cache)

        station_info = ""
        walk_min = 999
        nearest_station_name = ""
        nearest_station_line = ""
        if coord:
            nearest, dist_km, walk_min = find_nearest_station(coord["lat"], coord["lon"])
            nearest_station_name = nearest["name"]
            nearest_station_line = nearest["line"]
            if walk_min <= 15:
                station_info = f"ğŸš‡ {nearest['name']}ì—­ {walk_min}ë¶„"
            elif walk_min <= 25:
                station_info = f"ğŸšŒ {nearest['name']}ì—­ {walk_min}ë¶„"
            else:
                station_info = f"ğŸ“ ì—­ ë¨¼ ì§€ì—­"

        household_str = f"{household}ì„¸ëŒ€" if household > 0 else ""

        summaries.append({
            "ì•„íŒŒíŠ¸": group["ì•„íŒŒíŠ¸"],
            "ë²•ì •ë™": group["ë²•ì •ë™"],
            "ë©´ì ": group["ë©´ì "],
            "í‰": pyeong,
            "ê±´ì¶•ë…„ë„": group["ê±´ì¶•ë…„ë„"],
            "ê±´ìˆ˜": len(trades),
            "ìµœì €ê°€": min_p,
            "ìµœê³ ê°€": max_p,
            "í‰ê· ê°€": avg_p,
            "í‰ë‹¹ê°€": price_per_pyeong,
            "ì—­ì •ë³´": station_info,
            "ë„ë³´ë¶„": walk_min,
            "ì„¸ëŒ€ìˆ˜": household,
            "ì„¸ëŒ€ìˆ˜í‘œì‹œ": household_str,
        })

        # [v4] ê° ê±°ë˜ë¥¼ data.json ì €ì¥ìš©ìœ¼ë¡œ ì¤€ë¹„
        for t in trades:
            try:
                trade_date_str = f"{t['ê±°ë˜ë…„ë„']}-{t['ê±°ë˜ì›”']:02d}-{t['ê±°ë˜ì¼']:02d}"
            except (ValueError, TypeError):
                trade_date_str = ""

            search_query = urllib.parse.quote(f"{group['ë²•ì •ë™']} {group['ì•„íŒŒíŠ¸']}")
            naver_link = f"https://m.land.naver.com/search/result/{search_query}"

            data_items.append({
                "name": group["ì•„íŒŒíŠ¸"],
                "region": region_name,
                "dong": group["ë²•ì •ë™"],
                "area_m2": group["ë©´ì "],
                "area_py": pyeong,
                "price": t["ê±°ë˜ê¸ˆì•¡"],
                "price_per_py": round(t["ê±°ë˜ê¸ˆì•¡"] / pyeong) if pyeong > 0 else 0,
                "floor": t["ì¸µ"],
                "built_year": group["ê±´ì¶•ë…„ë„"],
                "households": household,
                "station": nearest_station_name,
                "line": nearest_station_line,
                "walk_min": walk_min if walk_min < 999 else None,
                "trade_date": trade_date_str,
                "link": naver_link,
                "regulated": False,  # ê¸°ë³¸ê°’, ë‚˜ì¤‘ì— ê·œì œì§€ì—­ íŒë³„ ì¶”ê°€ ê°€ëŠ¥
            })

    # ê°€ê²©ìˆœ ì •ë ¬
    summaries.sort(key=lambda x: x["í‰ê· ê°€"])

    # ê°€ê²©ëŒ€ë³„ ê·¸ë£¹í•‘
    price_groups = {}
    for s in summaries:
        label = price_group_label(s["í‰ê· ê°€"])
        if label not in price_groups:
            price_groups[label] = []
        price_groups[label].append(s)

    # ë©”ì‹œì§€ ìƒì„±
    total_trades = sum(s["ê±´ìˆ˜"] for s in summaries)
    total_complexes = len(summaries)

    lines = [
        f"ğŸ“ *{region_name}*",
        f"   {total_complexes}ê°œ ë‹¨ì§€ / {total_trades}ê±´ ê±°ë˜",
        ""
    ]

    for label in sorted(price_groups.keys()):
        items = price_groups[label]
        lines.append(f"ğŸ’° *{label}*")

        shown = items[:5]
        hidden = len(items) - 5

        for s in shown:
            price_str = format_price(s["ìµœì €ê°€"])
            if s["ê±´ìˆ˜"] > 1:
                price_str = f"{format_price(s['ìµœì €ê°€'])}~{format_price(s['ìµœê³ ê°€'])}"

            household_str = s["ì„¸ëŒ€ìˆ˜í‘œì‹œ"] if s["ì„¸ëŒ€ìˆ˜í‘œì‹œ"] else "-ì„¸ëŒ€"
            station_str = s["ì—­ì •ë³´"] if s["ì—­ì •ë³´"] else "ğŸ“ ì—­ì •ë³´ ì—†ìŒ"
            search_query = urllib.parse.quote(f"{s['ë²•ì •ë™']} {s['ì•„íŒŒíŠ¸']}")
            naver_link = f"https://m.land.naver.com/search/result/{search_query}"

            line = f"  â€¢ {s['ë²•ì •ë™']} {s['ì•„íŒŒíŠ¸']}"
            line += f"\n    {price_str} | {s['í‰']}í‰ | {s['ê±´ì¶•ë…„ë„']}ë…„ | {household_str} | {station_str}"
            if s["ê±´ìˆ˜"] > 1:
                line += f" | {s['ê±´ìˆ˜']}ê±´"
            line += f"\n    ğŸ”— [ë§¤ë¬¼ë³´ê¸°]({naver_link})"
            lines.append(line)

        if hidden > 0:
            lines.append(f"  â‹¯ ì™¸ {hidden}ê°œ ë‹¨ì§€")
        lines.append("")

    if skipped_small > 0:
        lines.append(f"â„¹ï¸ {min_households}ì„¸ëŒ€ ë¯¸ë§Œ {skipped_small}ê°œ ë‹¨ì§€ ì œì™¸")

    return "\n".join(lines), data_items


# â”€â”€â”€ í…”ë ˆê·¸ë¨ ì „ì†¡ â”€â”€â”€
def send_telegram(bot_token, chat_id, message):
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    payload = {"chat_id": chat_id, "text": message, "parse_mode": "Markdown"}
    try:
        resp = requests.post(url, json=payload, timeout=10)
        if resp.status_code != 200:
            print(f"  [ì˜¤ë¥˜] í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {resp.text}")
            return False
        return True
    except requests.exceptions.RequestException as e:
        print(f"  [ì˜¤ë¥˜] í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {e}")
        return False


def send_long_message(bot_token, chat_id, message):
    MAX_LEN = 4000
    if len(message) <= MAX_LEN:
        return send_telegram(bot_token, chat_id, message)

    lines = message.split("\n")
    chunk = ""
    success = True
    for line in lines:
        if len(chunk) + len(line) + 1 > MAX_LEN:
            if chunk:
                if not send_telegram(bot_token, chat_id, chunk):
                    success = False
                chunk = ""
        chunk += line + "\n"
    if chunk.strip():
        if not send_telegram(bot_token, chat_id, chunk):
            success = False
    return success


# â”€â”€â”€ ë©”ì¸ â”€â”€â”€
def main():
    print("=" * 50)
    print("ğŸ  ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ëª¨ë‹ˆí„°ë§ v4 (data.json)")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)

    config = load_config()
    api_key = config["api_key"]
    bot_token = config["telegram"]["bot_token"]
    chat_id = config["telegram"]["chat_id"]
    kakao_key = config.get("kakao_key", "")
    filters = config["filters"]
    regions = config["regions"]

    history = load_history()
    history_set = set(history)
    coord_cache = load_coord_cache()
    apt_info_cache = load_apt_info_cache()
    apt_list_cache = {}
    min_households = filters.get("min_households", 200)

    # [v4] ê¸°ì¡´ data.json ë¡œë“œ
    existing_data = load_data_json()
    existing_properties = existing_data.get("properties", [])

    # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ì¤‘ë³µ ì²´í¬ìš© í‚¤ ì„¸íŠ¸ ìƒì„±
    existing_keys = set()
    for p in existing_properties:
        key = f"{p['region']}_{p['name']}_{p['area_m2']}_{p['price']}_{p['floor']}_{p['trade_date']}"
        existing_keys.add(key)

    now = datetime.now()
    months = [now.strftime("%Y%m"), (now - timedelta(days=30)).strftime("%Y%m")]
    months = list(dict.fromkeys(months))

    total_new = 0
    total_checked = 0
    all_new_items = []  # [v4] ìƒˆë¡œ ì¶”ê°€í•  ë§¤ë¬¼ë“¤
    region_results = {}

    for region in regions:
        region_name = region["name"]
        region_code = region["code"]
        sgg_name = region.get("sgg_name", region_name)
        print(f"\nğŸ“ {region_name} ({region_code}) ì¡°íšŒ ì¤‘...")

        new_trades = []
        for month in months:
            print(f"  ğŸ“… {month} ë°ì´í„° ì¡°íšŒ...")
            trades = fetch_trades(api_key, region_code, month)
            print(f"  â†’ {len(trades)}ê±´ ì¡°íšŒë¨")
            filtered = filter_trades(trades, filters)
            total_checked += len(trades)
            print(f"  â†’ {len(filtered)}ê±´ í•„í„° í†µê³¼")

            for trade in filtered:
                trade_id = make_trade_id(trade, region_name)
                if trade_id in history_set:
                    continue
                new_trades.append(trade)
                history.append(trade_id)
                history_set.add(trade_id)
                total_new += 1

        if new_trades:
            region_results[region_name] = {"trades": new_trades, "sgg_name": sgg_name, "region_code": region_code}
            print(f"  âœ… ìƒˆ ê±°ë˜ {len(new_trades)}ê±´")

    # â”€â”€â”€ ê²€ìƒ‰ ì¡°ê±´ ìš”ì•½ í…ìŠ¤íŠ¸ â”€â”€â”€
    min_py = to_pyeong(filters["min_area"])
    max_py = to_pyeong(filters["max_area"])
    max_p = filters["max_price"]
    price_label = f"{max_p // 10000}ì–µ" if max_p >= 10000 else f"{max_p:,}ë§Œ"
    region_names = [r["name"] for r in regions]

    filter_text = (
        f"ğŸ” *ê²€ìƒ‰ ì¡°ê±´*\n"
        f"  ë©´ì : {filters['min_area']}~{filters['max_area']}ã¡ ({min_py}~{max_py}í‰)\n"
        f"  ê°€ê²©: ~{price_label} ì´í•˜\n"
        f"  ì¸µìˆ˜: {filters.get('min_floor', 1)}ì¸µ ì´ìƒ\n"
        f"  ì„¸ëŒ€ìˆ˜: {min_households}ì„¸ëŒ€ ì´ìƒ\n"
        f"  ì§€ì—­: {', '.join(region_names)}"
    )

    # â”€â”€â”€ ìˆ˜ì§‘ ê¸°ê°„ ê³„ì‚° â”€â”€â”€
    max_days = filters.get("max_days_ago", 14)
    date_from = (now - timedelta(days=max_days)).strftime("%m/%d")
    date_to = now.strftime("%m/%d")

    # â”€â”€â”€ í…”ë ˆê·¸ë¨ ì „ì†¡ â”€â”€â”€
    region_summary_lines = []
    for region in regions:
        rname = region["name"]
        if rname in region_results:
            count = len(region_results[rname]["trades"])
            region_summary_lines.append(f"  â€¢ {rname}: {count}ê±´")
        else:
            region_summary_lines.append(f"  â€¢ {rname}: 0ê±´")

    header = (
        f"ğŸ  *ë¶€ë™ì‚° ì‹¤ê±°ë˜ ë¦¬í¬íŠ¸*\n"
        f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
        f"â° {now.strftime('%Y-%m-%d %H:%M')}\n"
        f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {date_from} ~ {date_to} ê±°ë˜\n"
        f"ğŸ†• ì´ {total_new}ê±´ (ì‹ ê·œ ê±°ë˜)\n\n"
        + "\n".join(region_summary_lines) +
        f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
        + filter_text
    )
    send_telegram(bot_token, chat_id, header)

    if region_results:
        for rname, rdata in region_results.items():
            complex_groups = group_by_complex(rdata["trades"])
            message, data_items = build_region_summary(
                rname, complex_groups, kakao_key, coord_cache,
                rdata["sgg_name"], api_key, apt_info_cache,
                min_households, rdata["region_code"], apt_list_cache
            )
            send_long_message(bot_token, chat_id, message)
            print(f"  ğŸ“¤ {rname} ì•Œë¦¼ ì „ì†¡")

            # [v4] data.jsonìš© ì•„ì´í…œ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
            for item in data_items:
                item_key = f"{item['region']}_{item['name']}_{item['area_m2']}_{item['price']}_{item['floor']}_{item['trade_date']}"
                if item_key not in existing_keys:
                    all_new_items.append(item)
                    existing_keys.add(item_key)

    # [v4] data.json ì—…ë°ì´íŠ¸
    # ê¸°ì¡´ ë°ì´í„° + ì‹ ê·œ ë°ì´í„° í•©ì¹˜ê¸°
    all_properties = existing_properties + all_new_items

    # 90ì¼ ì´ìƒ ëœ ë°ì´í„° ì •ë¦¬ (ë„ˆë¬´ ì˜¤ë˜ëœ ê±´ ì œê±°)
    cutoff_date = (now - timedelta(days=90)).strftime("%Y-%m-%d")
    all_properties = [
        p for p in all_properties
        if p.get("trade_date", "9999") >= cutoff_date or not p.get("trade_date")
    ]

    # ê±°ë˜ì¼ ê¸°ì¤€ ìµœì‹ ìˆœ ì •ë ¬
    all_properties.sort(key=lambda x: x.get("trade_date", ""), reverse=True)

    data_json = {
        "updated_at": now.strftime("%Y-%m-%d %H:%M"),
        "total_count": len(all_properties),
        "new_count": len(all_new_items),
        "properties": all_properties
    }
    save_data_json(data_json)

    # ì €ì¥
    save_history(history)
    save_coord_cache(coord_cache)
    save_apt_info_cache(apt_info_cache)

    print(f"\n{'=' * 50}")
    print(f"âœ… ì™„ë£Œ! ìƒˆ ì•Œë¦¼ {total_new}ê±´ / data.json {len(all_properties)}ê±´")
    print(f"{'=' * 50}")


if __name__ == "__main__":
    main()
